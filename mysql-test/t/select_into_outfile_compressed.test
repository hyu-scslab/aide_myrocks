--exec export ZSTD=`command -v zstd`
--let $zstd_exists=ZSTD
if(!$zstd_exists)
{
  skip test requires zstd program to be installed in the system, skipping test;
}

 
--let $tmp_dir=`SELECT @@GLOBAL.secure_file_priv`


#
# create table with highly repeating data
# and dump it compressed
# the result file should be small
#  

CREATE TABLE t1(x VARCHAR(1000));
insert into t1 values (REPEAT("xyz", 100));
insert into t1 select t1.* from t1, t1 t12, t1 t13, t1 t14;
insert into t1 select t1.* from t1, t1 t12, t1 t13, t1 t14;
insert into t1 select t1.* from t1, t1 t12, t1 t13, t1 t14;

# dump data into file
--let $output_file=$tmp_dir/t1.txt
--replace_result $output_file OUTPUT_FILE
--eval SELECT * FROM t1 INTO OUTFILE '$output_file';
# dump data into file, compressed
--let $output_file_zstd=$tmp_dir/t1.txt.0.zst
--replace_result $output_file OUTPUT_FILE
--eval SELECT * FROM t1 INTO OUTFILE '$output_file' COMPRESSED;
# validate compressed output
--exec zstd -dcq $output_file_zstd | diff $output_file -

#
# create table with 100000 rows of random data with 80 byte rows
# and dump it compressed
# the result should be low compressed
#

truncate table t1;
# generate random input data for low compression
--exec dd bs=10000000 count=1 if=/dev/urandom | base64 -w 80 > $tmp_dir/t1.tmp
--let $input_file=$tmp_dir/t2.txt
--exec head -n 100000  $tmp_dir/t1.tmp > $input_file
--remove_file $tmp_dir/t1.tmp 
# load input data
--replace_result $input_file INPUT_FILE
--eval LOAD DATA LOCAL INFILE '$input_file' INTO TABLE t1;
# dump random data into file, compressed
--let $output_file2=$tmp_dir/t2.txt
--let $output_file_zstd2=$tmp_dir/t2.txt.0.zst
--replace_result $output_file2 OUTPUT_FILE2
--eval SELECT * FROM t1 INTO OUTFILE '$output_file2' COMPRESSED(0);
# validate
--exec zstd -dcq $output_file_zstd2 | diff $input_file - 


#
# check how compression works for empty table
#

truncate table t1;
# dump empty table
--let $output_file3=$tmp_dir/t3.txt
--let $output_file_zstd3=$tmp_dir/t3.txt.0.zst
--replace_result $output_file3 OUTPUT_FILE3
--eval SELECT * FROM t1 INTO OUTFILE '$output_file3' COMPRESSED;
# validate
--exec zstd -dcq $output_file_zstd3 | wc -l


#
# test how compression works for big rows (5000 bytes)
# which do not fit into internal cache buffer
# and require more than one write operation for a single row
#

create table t10(x varchar(5001));
# generate rows bigger 4K, so they cannot be flushed in one step 
--exec dd bs=10000000 count=1 if=/dev/urandom | base64 -w 5000 > $tmp_dir/t1.tmp
--let $input_file_big_row=$tmp_dir/t10.txt
--exec head -n 1000  $tmp_dir/t1.tmp > $input_file_big_row
--remove_file $tmp_dir/t1.tmp 
# load input data
--replace_result $input_file_big_row INPUT_FILE_BIG_ROW
--eval LOAD DATA LOCAL INFILE '$input_file_big_row' INTO TABLE t10;
# dump data into file, compressed
--let $output_file_big_row=$tmp_dir/t10.txt
--let $output_file_zstd_big_row=$tmp_dir/t10.txt.0.zst
--replace_result $output_file_big_row OUTPUT_FILE_BIG_ROW
--eval SELECT * FROM t10 INTO OUTFILE '$output_file_big_row' COMPRESSED;
# validate
--exec zstd -dcq $output_file_zstd_big_row | diff $input_file_big_row - 

set @saved_select_into_file_fsync_size = @@session.select_into_file_fsync_size;
set @@session.select_into_file_fsync_size = 67108864;

# dump data into file
--let $output_file4=$tmp_dir/t4.txt
--replace_result $output_file4 OUTPUT_FILE4
--eval SELECT 1 INTO OUTFILE '$output_file4';
# dump data into file, compressed
--let $output_file_zstd4=$tmp_dir/t4.txt.0.zst
--replace_result $output_file4 OUTPUT_FILE4
--eval SELECT 1 INTO OUTFILE '$output_file4' COMPRESSED;

set @@session.select_into_file_fsync_size = @saved_select_into_file_fsync_size;

# cleanup
--remove_file $output_file
--remove_file $output_file_zstd
--remove_file $input_file
--remove_file $output_file_zstd2
--remove_file $output_file_zstd3
--remove_file $input_file_big_row
--remove_file $output_file_zstd_big_row
--remove_file $output_file4
--remove_file $output_file_zstd4

DROP TABLE t1, t10;
